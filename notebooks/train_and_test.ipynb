{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# ASL Sign Language Detection — Training & Testing Guide\n",
                "\n",
                "This notebook walks you through the full pipeline:\n",
                "1. **Download** the ASL Alphabet dataset from Kaggle\n",
                "2. **Extract** hand landmarks using MediaPipe\n",
                "3. **Train** a Random Forest classifier\n",
                "4. **Evaluate** model performance\n",
                "5. **Predict** on a single image\n",
                "\n",
                "### Running this notebook\n",
                "You can run this notebook **locally** or use **Docker** for the heavy-lifting steps (extract/train).\n",
                "Docker and local share the same `data/` and `models/` folders via volume mounts, so you can mix both.\n",
                "\n",
                "> **Note:** Real-time webcam recognition must be run locally (not in a notebook or Docker).\n",
                "> After training, use `python main.py realtime` from your terminal."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 0. Setup\n",
                "\n",
                "Make sure you have installed the project dependencies:\n",
                "```bash\n",
                "pip install -r requirements.txt\n",
                "```\n",
                "\n",
                "**If you used Docker** for download/extract/train, the results are already in your local `data/` and `models/` folders (Docker mounts them via volumes). You can skip straight to the evaluation steps."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import sys\n",
                "\n",
                "# Ensure the project root is on the path\n",
                "PROJECT_ROOT = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
                "if PROJECT_ROOT not in sys.path:\n",
                "    sys.path.insert(0, PROJECT_ROOT)\n",
                "os.chdir(PROJECT_ROOT)\n",
                "\n",
                "print(f'Project root: {PROJECT_ROOT}')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Download the Dataset\n",
                "\n",
                "The dataset is the [ASL Alphabet](https://www.kaggle.com/datasets/grassknoted/asl-alphabet) from Kaggle (~1GB).\n",
                "\n",
                "**Prerequisites (one of):**\n",
                "- Place `kaggle.json` at `~/.kaggle/kaggle.json` (download from https://www.kaggle.com/settings → Create New Token)\n",
                "- Or set the environment variable: `export KAGGLE_API_TOKEN=your_token_here`\n",
                "\n",
                "**Already have the data?** If you downloaded via Docker (`docker compose run --rm download`), the data is already in `data/` — skip this step."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Check if dataset already exists\n",
                "DATA_DIR = 'data/asl_alphabet_train/asl_alphabet_train'\n",
                "\n",
                "if os.path.exists(DATA_DIR):\n",
                "    classes = sorted([d for d in os.listdir(DATA_DIR) if os.path.isdir(os.path.join(DATA_DIR, d))])\n",
                "    print(f'Dataset already downloaded! Found {len(classes)} classes: {classes}')\n",
                "else:\n",
                "    print('Dataset not found. Run the cell below to download it.')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Download the dataset (only run if not already present)\n",
                "if not os.path.exists(DATA_DIR):\n",
                "    !python main.py download\n",
                "else:\n",
                "    print('Skipping download — dataset already exists.')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Explore the Dataset\n",
                "\n",
                "Let's look at a few sample images from different classes."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import cv2\n",
                "import matplotlib.pyplot as plt\n",
                "import random\n",
                "\n",
                "classes = sorted([d for d in os.listdir(DATA_DIR) if os.path.isdir(os.path.join(DATA_DIR, d))])\n",
                "\n",
                "# Show one random sample from 8 random classes\n",
                "sample_classes = random.sample(classes, min(8, len(classes)))\n",
                "\n",
                "fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
                "for ax, cls in zip(axes.flat, sample_classes):\n",
                "    cls_dir = os.path.join(DATA_DIR, cls)\n",
                "    imgs = [f for f in os.listdir(cls_dir) if f.lower().endswith(('.jpg', '.png'))]\n",
                "    img_path = os.path.join(cls_dir, random.choice(imgs))\n",
                "    img = cv2.imread(img_path)\n",
                "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
                "    ax.imshow(img)\n",
                "    ax.set_title(f'Class: {cls}', fontsize=14, fontweight='bold')\n",
                "    ax.axis('off')\n",
                "\n",
                "plt.suptitle('Sample Images from ASL Alphabet Dataset', fontsize=16)\n",
                "plt.tight_layout()\n",
                "plt.show()\n",
                "\n",
                "# Count images per class\n",
                "print(f'\\nTotal classes: {len(classes)}')\n",
                "for cls in classes:\n",
                "    count = len(os.listdir(os.path.join(DATA_DIR, cls)))\n",
                "    print(f'  {cls}: {count} images')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Extract Features\n",
                "\n",
                "This step uses MediaPipe to detect hand landmarks in each image, then computes:\n",
                "- **63 base features**: 21 hand landmarks × 3 coordinates (x, y, z)\n",
                "- **54 engineered features**: distances, angles, z-depth comparisons, etc.\n",
                "\n",
                "The result is saved to `data/landmarks.pkl`.\n",
                "\n",
                "⏱ **Time estimate:**\n",
                "- Full dataset (~3000 images/class): ~15-20 minutes\n",
                "- Quick test (200 images/class): ~2-3 minutes"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from src.extract_features import extract_features_from_dataset\n",
                "\n",
                "FEATURES_PATH = 'data/landmarks.pkl'\n",
                "\n",
                "# Set to None for full dataset, or a number like 200 for a quick test\n",
                "SAMPLE_PER_CLASS = None  # Change to 200 for a quick run\n",
                "\n",
                "if os.path.exists(FEATURES_PATH):\n",
                "    print(f'Features already extracted at {FEATURES_PATH}.')\n",
                "    print('Delete the file and re-run this cell to re-extract.')\n",
                "else:\n",
                "    features, labels = extract_features_from_dataset(\n",
                "        DATA_DIR, FEATURES_PATH, sample_per_class=SAMPLE_PER_CLASS\n",
                "    )\n",
                "    print(f'\\nDone! Extracted {len(features)} samples with {features.shape[1]} features each.')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Train the Model\n",
                "\n",
                "We train a **Random Forest** classifier (100 trees) on the extracted features.\n",
                "The model is saved to `models/asl_classifier.pkl`."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from src.train import train_classifier\n",
                "\n",
                "MODEL_PATH = 'models/asl_classifier.pkl'\n",
                "\n",
                "clf, accuracy = train_classifier(FEATURES_PATH, MODEL_PATH)\n",
                "print(f'\\n✅ Model trained with {accuracy * 100:.2f}% accuracy')\n",
                "print(f'   Saved to: {MODEL_PATH}')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Evaluate Model Performance\n",
                "\n",
                "Let's visualize the confusion matrix and feature importances."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pickle\n",
                "import numpy as np\n",
                "from sklearn.model_selection import train_test_split\n",
                "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report\n",
                "\n",
                "# Load features and model\n",
                "with open(FEATURES_PATH, 'rb') as f:\n",
                "    data = pickle.load(f)\n",
                "features, labels, classes = data['features'], data['labels'], data['classes']\n",
                "\n",
                "with open(MODEL_PATH, 'rb') as f:\n",
                "    model_data = pickle.load(f)\n",
                "clf = model_data['classifier']\n",
                "\n",
                "# Split (same seed as training)\n",
                "X_train, X_test, y_train, y_test = train_test_split(\n",
                "    features, labels, test_size=0.2, random_state=42, stratify=labels\n",
                ")\n",
                "y_pred = clf.predict(X_test)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Confusion Matrix\n",
                "fig, ax = plt.subplots(figsize=(18, 15))\n",
                "cm = confusion_matrix(y_test, y_pred, labels=classes)\n",
                "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=classes)\n",
                "disp.plot(ax=ax, cmap='Blues', xticks_rotation=45)\n",
                "ax.set_title('Confusion Matrix', fontsize=16)\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Per-class accuracy\n",
                "report = classification_report(y_test, y_pred, output_dict=True)\n",
                "\n",
                "class_acc = {cls: report[cls]['f1-score'] for cls in classes if cls in report}\n",
                "sorted_acc = sorted(class_acc.items(), key=lambda x: x[1])\n",
                "\n",
                "fig, ax = plt.subplots(figsize=(12, 8))\n",
                "names, scores = zip(*sorted_acc)\n",
                "colors = ['#e74c3c' if s < 0.9 else '#f39c12' if s < 0.95 else '#2ecc71' for s in scores]\n",
                "ax.barh(names, scores, color=colors)\n",
                "ax.set_xlim(0.5, 1.0)\n",
                "ax.set_xlabel('F1 Score', fontsize=12)\n",
                "ax.set_title('Per-Class F1 Score (red < 90%, yellow < 95%, green ≥ 95%)', fontsize=14)\n",
                "ax.axvline(x=0.95, color='gray', linestyle='--', alpha=0.5)\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Top 20 most important features\n",
                "importances = clf.feature_importances_\n",
                "\n",
                "# Name the features\n",
                "feature_names = []\n",
                "landmarks = ['Wrist','Thumb_CMC','Thumb_MCP','Thumb_IP','Thumb_Tip',\n",
                "              'Index_MCP','Index_PIP','Index_DIP','Index_Tip',\n",
                "              'Middle_MCP','Middle_PIP','Middle_DIP','Middle_Tip',\n",
                "              'Ring_MCP','Ring_PIP','Ring_DIP','Ring_Tip',\n",
                "              'Pinky_MCP','Pinky_PIP','Pinky_DIP','Pinky_Tip']\n",
                "for lm in landmarks:\n",
                "    for coord in ['x', 'y', 'z']:\n",
                "        feature_names.append(f'{lm}_{coord}')\n",
                "\n",
                "# Engineered feature names (simplified)\n",
                "eng_names = (\n",
                "    [f'thumb_to_{f}_dist' for f in ['index','middle','ring','pinky']] +\n",
                "    [f'adj_dist_{i}' for i in range(4)] +\n",
                "    [f'z_depth_{f}' for f in ['index','middle','ring','pinky']] +\n",
                "    ['z_thumb_palm'] +\n",
                "    [f'curl_{f}' for f in ['thumb','index','middle','ring','pinky']] +\n",
                "    [f'spread_{i}' for i in range(4)] +\n",
                "    ['thumb_idx_x','thumb_idx_y','thumb_idx_z'] +\n",
                "    [f'tip_wrist_{f}' for f in ['thumb','index','middle','ring','pinky']] +\n",
                "    ['cross_z'] +\n",
                "    [f'thumb_pip_{f}' for f in ['index','middle','ring','pinky']] +\n",
                "    [f'thumb_dip_{f}' for f in ['index','middle','ring','pinky']] +\n",
                "    [f'y_drape_{f}' for f in ['index','middle','ring','pinky']] +\n",
                "    ['thumb_mid_x','thumb_mid_y','thumb_mid_z'] +\n",
                "    ['thumb_ring_x','thumb_ring_y','thumb_ring_z'] +\n",
                "    ['drape_score','palm_plane_dist'] +\n",
                "    [f'dip_curl_{f}' for f in ['index','middle','ring','pinky']]\n",
                ")\n",
                "feature_names.extend(eng_names)\n",
                "\n",
                "# Trim or pad to match\n",
                "feature_names = feature_names[:len(importances)]\n",
                "\n",
                "top_n = 20\n",
                "indices = np.argsort(importances)[-top_n:]\n",
                "\n",
                "fig, ax = plt.subplots(figsize=(10, 8))\n",
                "ax.barh([feature_names[i] for i in indices], importances[indices], color='steelblue')\n",
                "ax.set_xlabel('Feature Importance', fontsize=12)\n",
                "ax.set_title(f'Top {top_n} Most Important Features', fontsize=14)\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Predict on a Single Image\n",
                "\n",
                "Test the trained model on any image of a hand sign."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from src.extract_features import HandLandmarkExtractor\n",
                "\n",
                "# Pick a random test image\n",
                "test_class = random.choice(classes)\n",
                "test_dir = os.path.join(DATA_DIR, test_class)\n",
                "test_img_name = random.choice(os.listdir(test_dir))\n",
                "test_img_path = os.path.join(test_dir, test_img_name)\n",
                "\n",
                "print(f'Testing with: {test_img_path}')\n",
                "print(f'True label: {test_class}')\n",
                "\n",
                "# Load and predict\n",
                "image = cv2.imread(test_img_path)\n",
                "extractor = HandLandmarkExtractor(use_engineered_features=True)\n",
                "features = extractor.extract_landmarks(image)\n",
                "extractor.close()\n",
                "\n",
                "if features is not None:\n",
                "    features = features.reshape(1, -1)\n",
                "    prediction = clf.predict(features)[0]\n",
                "    probs = clf.predict_proba(features)[0]\n",
                "    confidence = np.max(probs)\n",
                "\n",
                "    # Display\n",
                "    fig, ax = plt.subplots(figsize=(6, 6))\n",
                "    ax.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
                "    color = 'green' if prediction == test_class else 'red'\n",
                "    ax.set_title(f'Predicted: {prediction} ({confidence*100:.1f}%)\\nTrue: {test_class}',\n",
                "                 fontsize=14, color=color, fontweight='bold')\n",
                "    ax.axis('off')\n",
                "    plt.show()\n",
                "else:\n",
                "    print('No hand detected in this image. Try another one.')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Next Steps\n",
                "\n",
                "Now that you have a trained model, you can:\n",
                "\n",
                "### Real-time Webcam Recognition (run locally)\n",
                "```bash\n",
                "source venv/bin/activate\n",
                "python main.py realtime         # Letter-by-letter mode\n",
                "python main.py realtime --spell  # Spell mode (words)\n",
                "```\n",
                "\n",
                "### Docker alternative for training\n",
                "If you prefer Docker, the `data/` and `models/` folders are shared via volume mounts:\n",
                "```bash\n",
                "docker compose run --rm extract-quick  # Extract features\n",
                "docker compose run --rm train          # Train the model\n",
                "```\n",
                "The trained model ends up in the same `models/` folder, so you can run `realtime` locally right after.\n",
                "\n",
                "### Improve accuracy\n",
                "- Re-extract with full dataset: set `SAMPLE_PER_CLASS = None` above\n",
                "- Try different classifiers in `src/train.py` (SVM, XGBoost, etc.)\n",
                "- See `PLAN.md` for more ideas"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}